{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import os\n",
    "import pybullet_envs\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import pickle\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Replay:\n",
    "    def __init__(self, max_memory=1_000_000):\n",
    "        self.memory = deque(maxlen= max_memory)\n",
    "    \n",
    "    def add_memory(self, trans):\n",
    "        self.memory.append(trans)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        idx=np.random.randint(0,len(self.memory),batch_size)\n",
    "        batch=random.sample(self.memory, batch_size)\n",
    "        batch_next_states, batch_rewards, batch_dones, batch_states, batch_actions= [], [], [], [], []\n",
    "        \n",
    "        for next_state, reward, done, state, action in batch:\n",
    "            batch_next_states.append(np.array(next_state, copy=False))\n",
    "            batch_rewards.append(np.array(reward, copy=False))\n",
    "            batch_dones.append(np.array(done, copy=False))\n",
    "            batch_states.append(np.array(state, copy=False))\n",
    "            batch_actions.append(np.array(action, copy=False))\n",
    "            \n",
    "        return np.array(batch_next_states), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1), np.array(batch_states),  np.array(batch_actions)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        self.layer1 = nn.Linear(state_size, 400)\n",
    "        self.layer2 = nn.Linear(400, 300)\n",
    "        self.layer3 = nn.Linear(300, action_size)\n",
    "        self.max_action = max_action\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = self.max_action * torch.tanh(self.layer3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.layer11 = nn.Linear(state_size + action_size, 400)\n",
    "        self.layer12 = nn.Linear(400, 300)\n",
    "        self.layer13 = nn.Linear(300, 1)\n",
    "        \n",
    "        self.layer21 = nn.Linear(state_size + action_size, 400)\n",
    "        self.layer22 = nn.Linear(400, 300)\n",
    "        self.layer23 = nn.Linear(300, 1)\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        x12 = torch.cat([x1, x2], 1)\n",
    "        \n",
    "        x1 = F.relu(self.layer11(x12))\n",
    "        x1 = F.relu(self.layer12(x1))\n",
    "        x1 = self.layer13(x1)\n",
    "        \n",
    "        x2 = F.relu(self.layer21(x12))\n",
    "        x2 = F.relu(self.layer22(x2))\n",
    "        x2 = self.layer23(x2)\n",
    "        return x1, x2\n",
    "    \n",
    "    def Q1(self, x1, x2):\n",
    "        x12 = torch.cat([x1, x2], 1)\n",
    "        \n",
    "        x1 = F.relu(self.layer11(x12))\n",
    "        x1 = F.relu(self.layer12(x1))\n",
    "        x1 = self.layer13(x1)\n",
    "        return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3:\n",
    "    def __init__(self, state_size, action_size, max_action):\n",
    "        self.actor = Actor(state_size, action_size, max_action).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, max_action).to(device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
    "        \n",
    "        self.critic = Critic(state_size, action_size).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size).to(device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
    "        \n",
    "        self.max_action = max_action\n",
    "        self.discount = 0.99\n",
    "        self.batch_size = 100\n",
    "        self.tau = 0.005\n",
    "        self.policy_noise = 0.2\n",
    "        self.noise_clip = 0.5\n",
    "        self.policy_freq = 2\n",
    "        \n",
    "    def action(self, state):\n",
    "        state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
    "        return self.actor(state).cpu().data.numpy().flatten()\n",
    "    \n",
    "    def learn(self, iterations):\n",
    "        for it in range(iterations):\n",
    "            batch_next_state, batch_reward, batch_done, batch_state, batch_action = replay.sample(self.batch_size)\n",
    "            \n",
    "            next_state = torch.Tensor(batch_next_state).to(device)\n",
    "            reward = torch.Tensor(batch_reward).to(device)\n",
    "            done = torch.Tensor(batch_done).to(device)\n",
    "            state = torch.Tensor(batch_state).to(device)\n",
    "            action = torch.Tensor(batch_action).to(device)\n",
    "            \n",
    "            next_action = self.actor_target.forward(next_state)\n",
    "\n",
    "            noise = torch.Tensor(batch_action).data.normal_(0, self.policy_noise).to(device)\n",
    "            noise = noise.clamp(-self.noise_clip, self.noise_clip)\n",
    "            next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
    "\n",
    "            target_Q1, target_Q2 = self.critic_target.forward(next_state, next_action)\n",
    "            target_Q = torch.min(target_Q1, target_Q2)\n",
    "            target_Q = reward + ((1 - done) * self.discount * target_Q).detach()\n",
    "\n",
    "            current_Q1, current_Q2 = self.critic(state, action)\n",
    "\n",
    "            critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            self.critic_optimizer.step()\n",
    "\n",
    "            if it % self.policy_freq == 0:\n",
    "                actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                self.actor_optimizer.step()\n",
    "\n",
    "                for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                    target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "                for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                    target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "    def save(self, filename, directory):\n",
    "        torch.save(self.actor.state_dict(), f'{directory}/{filename}_actor.pth')\n",
    "        torch.save(self.critic.state_dict(), f'{directory}/{filename}_critic.pth')\n",
    "\n",
    "    def load(self, filename, directory):\n",
    "        self.actor.load_state_dict(torch.load(f'{directory}/{filename}_actor.pth'))\n",
    "        self.critic.load_state_dict(torch.load(f'{directory}/{filename}_critic.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(policy, eval_episodes=10):\n",
    "    avg_reward = 0\n",
    "    for episode in range(eval_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        for step in range(env.spec.max_episode_steps):\n",
    "            action = policy.action(np.array(state))\n",
    "            state, reward, done, info = env.step(action)\n",
    "            avg_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "    avg_reward /= eval_episodes\n",
    "    state = env.reset()\n",
    "    print (\"---------------------------------------\")\n",
    "    print (f\"Policy Evaluation | Average Reward= {int(avg_reward)}\")\n",
    "    print (\"---------------------------------------\")\n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env_name='BipedalWalker-v3'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "folder=f'TD3_{env_name}'\n",
    "\n",
    "save_models = True\n",
    "save_video = False\n",
    "\n",
    "if not os.path.exists(f\"./{folder}/results\"):\n",
    "    os.makedirs(f\"./{folder}/results\")\n",
    "if save_models and not os.path.exists(f\"./{folder}/pytorch_models\"):\n",
    "    os.makedirs(f\"./{folder}/pytorch_models\")\n",
    "if save_video:\n",
    "    if not os.path.exists(f\"./{folder}/video\"):\n",
    "        os.makedirs(f\"./{folder}/video\")\n",
    "\n",
    "    env = wrappers.Monitor(env, f\"./{folder}/video\", force = False)\n",
    "    env.reset()\n",
    "\n",
    "seed=0\n",
    "env.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "\n",
    "max_episode_steps = env.spec.max_episode_steps\n",
    "max_episode=1100\n",
    "expl_noise = 0.1\n",
    "change_action = 100\n",
    "\n",
    "policy = TD3(state_size, action_size, max_action)\n",
    "replay = Replay()\n",
    "evaluations = []\n",
    "reward_lst=[]\n",
    "rewards=0\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for episode in range(max_episode):\n",
    "        state = env.reset()\n",
    "        episode_reward=0\n",
    "        \n",
    "        for step in range(max_episode_steps):\n",
    "            if episode < change_action:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = policy.action(np.array(state))\n",
    "                action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0]))\n",
    "                action = action.clip(env.action_space.low, env.action_space.high)\n",
    "                \n",
    "            new_state, reward, done, info = env.step(action)\n",
    "\n",
    "            replay.add_memory((new_state, reward, float(done), state, action))\n",
    "\n",
    "            episode_reward += reward\n",
    "            state = new_state\n",
    "\n",
    "            if done or step==max_episode_steps-1:\n",
    "                if episode != 0:\n",
    "                    print(f\"Episode: {episode:4} | Timesteps: {step:4} | Reward: {int(episode_reward):4}\")\n",
    "                    policy.learn(step)\n",
    "\n",
    "                if episode%50==0:\n",
    "                    evaluations.append(evaluate_policy(policy))\n",
    "                    policy.save(env_name, directory=f\"./{folder}/pytorch_models\")\n",
    "                    np.save(f\"./{folder}/results/{env_name}\", evaluations)\n",
    "                break\n",
    "                \n",
    "        reward_lst.append(int(episode_reward))\n",
    "   \n",
    "    env.close()\n",
    "    \n",
    "    with open(f\"./{folder}/reward_lst.txt\", \"wb\") as fp:  \n",
    "        pickle.dump(reward_lst, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save video\n",
    "# # policy.load(env_name, directory=f\"./{folder}/pytorch_models\")\n",
    "# env = wrappers.Monitor(env, f\"./{folder}/video\", force = True)\n",
    "# env.reset()\n",
    "# evaluate_policy(policy)\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load reward list\n",
    "# with open(f\"./{folder}/reward_lst.txt\", \"rb\") as fp:\n",
    "#     reward_lst = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_roll_lst=[]\n",
    "reward_roll=deque(maxlen=100)\n",
    "for i in reward_lst:\n",
    "    reward_roll.append(i)\n",
    "    reward_roll_lst.append(np.mean(reward_roll))\n",
    "\n",
    "fig= plt.figure(figsize=(12,8))\n",
    "plt.plot(list(range(len(reward_lst))), reward_lst, color='blue', alpha=0.5)\n",
    "plt.plot(list(range(len(reward_roll_lst))), reward_roll_lst, color='red', alpha=0.8)\n",
    "plt.plot(list(range(len(reward_roll_lst))), [300]*len(reward_roll_lst), color='green', alpha=0.6)\n",
    "\n",
    "plt.title(f'{env_name} reward using TD3', fontsize=16)\n",
    "plt.xlabel('Episode', fontsize=16)\n",
    "plt.ylabel('Reward', fontsize=16)\n",
    "plt.legend(('Reward','Rolling Reward (50)'), fontsize=14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ailabkernel1",
   "language": "python",
   "name": "ailabkernel1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
